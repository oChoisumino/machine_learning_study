{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 트리 알고리즘"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-1. 결정 트리\n",
    "* 로지스틱 회귀로 와인 분류하기\n",
    "  - info() : 데이터프레임의 요약된 정보 출력\n",
    "  - describe() : 데이터프레임 열의 통계값 제공\n",
    "  - LogisticRegression()\n",
    "    * 선형 분류 알고리즘인 로지스틱 회귀를 위한 클래스 생성\n",
    "    * sklearn.linear_model 내에 존재\n",
    "    * 매개변수\n",
    "      - solver : 사용할 알고리즘 선택\n",
    "        * lbfgs (default)\n",
    "        * sag (saga) : 확률적 평균 경사 하강법 알고리즘\n",
    "      - penalty : 규제 방식 선택\n",
    "        * l1 : l1 규제(라쏘 방식)\n",
    "        * l2 : l2 규제 (릿지 방식) -> default\n",
    "      - c : 규제의 강도 제어\n",
    "        * default 1.0\n",
    "    \n",
    "* 결정 트리\n",
    "  - 예/아니오에 대한 질문을 이어가면서 정답을 찾아 학습하는 알고리즘\n",
    "  - DecisionTreeClassifier()\n",
    "    * 결정 트리 분류 클래스를 생성\n",
    "    * sklearn.tree 내에 존재\n",
    "    * 매개변수\n",
    "      - criterion : 불순도를 지정 (노드에서 데이터를 분할할 기준을 정함)\n",
    "        * gini : 지니 불순도 (default)\n",
    "        * entropy : 엔트로피 불순도\n",
    "      - splitter : 노드를 분할하는 전략 선택\n",
    "        * best : 정보 이득이 최대가 되도록 분할\n",
    "        * random : 임의의 노드를 분할\n",
    "  - plot_tree()\n",
    "    * 결정 트리 모델을 시각화\n",
    "    * 매개변수\n",
    "      - max_depth : 나타낼 트리의 깊이 지정\n",
    "      - feature_names : 특성의 이름 지정\n",
    "      - filled : 타깃값에 따라 노드안에 색을 채운다.\n",
    "\n",
    "* 불순도\n",
    "  - 결정 트리가 최적의 질문을 찾기 위한 기준\n",
    "  - 노드에서 데이터를 분할할 기준을 정하는 것\n",
    "  - 지니 불순도 \n",
    "    * $$지니 불순도 = 1 - (음성 클래스 비율^2 + 양성 클래스 비율^2)$$\n",
    "  - 정보 이득\n",
    "    * 부모 노드와 자식 노드의 불순도 차이\n",
    "    * 결정 트리 알고리즘은 정보 이득이 최대화 되도록 학습\n",
    "    * $$부모 불순도 - (왼쪽 노드 샘플/부모의 샘플)*왼쪽 노드 불순도 - (오른쪽 노드 샘플/부모의 샘플)*오른쪽 노드 불순도$$\n",
    "  - 엔트로피 불순도\n",
    "    * $$엔트로피 불순도 = -음성 클래스 비율 * log_2(음성 클래스 비율) - 양성 클래스 비율 * log_2(양성 클래스 비율)$$\n",
    "\n",
    "* 가지치기\n",
    "  - 결정 트리의 성장을 제한하는 방법"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-2. 교차 검증과 그리드 서치\n",
    "* 검증 세트\n",
    "  - 하이퍼파라미터 튜닝을 위해 모델을 평가할 때, 테스트 세트를 사용하지 않기 위해 훈련세트에서 다시 떼어낸 데이터 세트\n",
    "\n",
    "* 교차 검증\n",
    "  - 훈련 세트를 여러 폴드로 나눈 다음 한 폴드가 검증 세트의 역할을 하고 나머지 폴드에서는 모델을 훈련한다. 이런 식으로 모든 폴드에 대해 검증 점수를 얻어 평균하는 방법\n",
    "  - 교차 검증의 경우, 검증 세트가 줄어들지만 각 폴드에서 계산한 검증 점수를 평균하기 때문에 안정된 점수로 생각 가능함\n",
    "  - cross_validate()\n",
    "    * 사이킷런에서 제공하는 교차 검증 함수\n",
    "    * sklearn.model_selection 내에 존재\n",
    "    * 평가할 모델 객체와 훈련세트 전체를 매개변수로 전달\n",
    "    * fit_time, score_time, test_score 키를 가진 딕셔너리 반환\n",
    "    * 매개변수\n",
    "      - scoring : 검증에 사용할 평가 지표\n",
    "        * default : 분류: 정확도를 의미하는 accuracy, 회귀: 결정계수를 의미하는 r2\n",
    "      - cv : 교차 검증 폴드 수나 스플리터 객체 지정\n",
    "        * default : 5, 분류: StratifiedKFold, 회귀: KFold로 5 폴드 교차 검증 수행\n",
    "      - n_jobs : 교차 검증을 수행할 때 사용할 CPU 코어수 지정 (default -1)\n",
    "      - return_train_score : 훈련 세트의 점수도 반환할 지의 여부 (default: False)\n",
    "\n",
    "* 하이퍼파라미터 튜닝\n",
    "  - 모델이 학습할 수 없어서 사용자가 지정해야만 하는 파라미터\n",
    "  - 각 매개변수를 동시에 바꿔가며 최적의 값을 찾는다\n",
    "  - 하이퍼파라미터 튜닝 절차\n",
    "    * 1. 라이브러리가 제공하는 기본값을 그대로 사용해 모델 훈련\n",
    "    * 2. 매개변수를 바꿔가며 교차 검증\n",
    "  - 방법\n",
    "    * 그리드 서치\n",
    "    * 랜덤 서치\n",
    "\n",
    "* 그리드 서치(Grid Search)\n",
    "  - 교차 검증으로 하이퍼파라미터 탐색을 수행\n",
    "  - 최상의 모델을 찾은 후 훈련 세트 전체를 사용해 최종 모델을 훈련\n",
    "  - 그리드 서치를 수행할 모델 객체와 탐색할 모델의 매개변수와 값을 전달\n",
    "  - 매개변수\n",
    "    * scoring : 검증에 사용할 평가 지표 지정\n",
    "      - default : 분류는 accuracy, 회귀는 r2\n",
    "    * cv : 교차 검증 폴드 수나 스플리터 객체를 지정\n",
    "      - default : 5, 분류는 StratifiedKFold, 회귀는 KFold\n",
    "    * n_jobs : 교차 검증을 수행할 때 사용할 CPU 코어수 지정\n",
    "    * return_train_score : 훈련 세트의 점수도 반환\n",
    "  - 절차 \n",
    "    * 1. 탐색할 매개변수 지정\n",
    "    * 2. 훈련세트에서 그리드 서치를 수행하여 최상의 평균 검증 점수가 나오는 매개변수 조합을 찾는다.\n",
    "    * 3. 그리드 서치는 최상의 매개변수에서 전체 훈련 세트를 사용해 최종 모델 훈련\n",
    "  - GridSearchCV()\n",
    "    * 하이퍼파라미터 탐색과 교차 검증을 한번에 수행\n",
    "    * 속성 \n",
    "      - best_params_ : 그리드 서치로 찾은 최적의 매개변수가 저장됨\n",
    "      - cv_results_ : mean_test_score : 각 매개변수에서 수행한 교차 검증의 평균 점수 저장\n",
    "      - best_estimator_ : 최상의 매개변수로 훈련한 모델 저장\n",
    "\n",
    "* 랜덤 서치(Random Search)\n",
    "  - 교차 검증으로 랜덤한 하이퍼파라미터 탐색을 수행\n",
    "  - 최상의 모델을 찾은 후 훈련 세트 전체를 사용해 최종 모델을 훈련\n",
    "  - 매개변수로 랜덤 서치를 수행할 모델 객체와 탐색할 모델의 매개변수와 확률 분포 객체를 전달\n",
    "  - 매개변수\n",
    "    * scoring : 검증에 사용할 평가 지표 지정\n",
    "      - default : 분류는 accuracy, 회귀는 r2\n",
    "    * cv : 교차 검증 폴드 수나 스플리터 객체를 지정\n",
    "      - default : 5, 분류는 StratifiedKFold, 회귀는 KFold\n",
    "    * n_jobs : 교차 검증을 수행할 때 사용할 CPU 코어수 지정\n",
    "    * return_train_score : 훈련 세트의 점수도 반환\n",
    "  \n",
    "* 값 뽑기\n",
    "  - uniform\n",
    "    * scipy.stats 패키지에 존재\n",
    "    * 주어진 범위에서 실수값을 뽑는다.\n",
    "  - randint\n",
    "    * scipy.stats 패키지에 존재\n",
    "    * 주어진 범위에서 정수값을 뽑는다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-3. 트리의 앙상블\n",
    "* 정형 데이터와 비정형 데이터\n",
    "  - 정형 데이터 : 구조화된 데이터\n",
    "    * 엑셀, csv, 데이터베이스에 저장하기 쉬운 구조의 데이터\n",
    "  - 비정형 데이터 : 구조화 되지 않은 데이터\n",
    "    * 엑셀, csv, 데이터베이스에 저장하기 어려운 구조의 데이터\n",
    "\n",
    "* 앙상블 학습\n",
    "  - 더 좋은 예측 결과를 만들기 위해 여러 개의 모델을 훈련하는 머신러닝 알고리즘\n",
    "\n",
    "* 부트스트랩(Bootstrap)\n",
    "  - 복원 추출로 표본을 뽑고, 각각의 표본에서 예측치를 계산하는 방식\n",
    "  - 랜덤 복원 추출\n",
    "\n",
    "* 랜덤 포레스트 (Random Forest)\n",
    "  - 대표적인 결정 트리 기반의 앙상블 학습방법\n",
    "  - 부트스트랩 샘플을 사용해 랜덤하게 일부 특성을 선택하여 트리를 만드는 것이 특징\n",
    "  - 결정 트리를 랜덤하게 만들어 결정트리의 숲을 만든다. 그리고 각 결정트리의 예측을 사용해 최종 예측을 만든다.\n",
    "  - RandomForestClassifier()\n",
    "    * 랜덤포레스트 분류 클래스\n",
    "    * sklearn.ensemble 내에 존재\n",
    "    * 매개변수\n",
    "      - n_estimators : 앙상블을 구성할 트리 개수 지정 (default 100)\n",
    "      - criterion : 불순도 지정 (default : gini)\n",
    "      - max_depth : 트리가 성장할 최대 깊이 지정 (default : None)\n",
    "      - min_samples_split : 노드를 나누기 위한 최소 샘플 개수 (default 2)\n",
    "      - max_features : 최적의 분할을 위해 탐색할 특성 개수 지정 (default auto로 특성 개수의 제곱근)\n",
    "      - bootstrap : 부트스트랩 샘플을 사용할 지 결정 (default True)\n",
    "      - oob_score : OOB 샘플을 사용하여 훈련한 모델을 평가할지 지정 (default : False)\n",
    "      - n_jobs : 사용할 CPU 코어수 지정 (default -1)\n",
    "\n",
    "* 엑스트라 트리\n",
    "  - 랜덤 포레스트와 비슷하게 결정 트리를 사용하여 앙상블 모델을 만든다.\n",
    "  - 결정 트리를 만들 때 부트스트랩 샘플을 사용하지 않고 전체 훈련 세트를 사용\n",
    "  - 랜덤하게 노드를 분할해 과대적합을 감소시킨다.\n",
    "  - 하나의 결정 트리에서 특성을 무작위로 분할하면 성능은 낮지만, 많은 트리를 앙상블하므로 과대적합을 막고 검증세트의 점수를 높이는 효과가 있다.\n",
    "  - ExtraTreeClassifier()\n",
    "    * 엑스트라 트리 분류 클래스\n",
    "    * sklearn.ensemble 내에 존재\n",
    "    * 매개변수\n",
    "      - n_estimators : 앙상블을 구성할 트리 개수 지정 (default 100)\n",
    "      - criterion : 불순도 지정 (default : gini)\n",
    "      - max_depth : 트리가 성장할 최대 깊이 지정 (default : None)\n",
    "      - min_samples_split : 노드를 나누기 위한 최소 샘플 개수 (default 2)\n",
    "      - max_features : 최적의 분할을 위해 탐색할 특성 개수 지정 (default auto로 특성 개수의 제곱근)\n",
    "      - bootstrap : 부트스트랩 샘플을 사용할 지 결정 (default False)\n",
    "      - oob_score : OOB 샘플을 사용하여 훈련한 모델을 평가할지 지정 (default : False)\n",
    "      - n_jobs : 사용할 CPU 코어수 지정 (default -1)\n",
    "\n",
    "* 그레이디언트 부스팅\n",
    "  - 깊이가 얕은 결정 트리를 사용해 이진 트리의 오차를 보완하는 방식으로 앙상블하는 방법\n",
    "  - 결정 트리를 연속적으로 추가하여 손실 함수를 최소화하는 앙상블 방법\n",
    "  - 훈련 속도는 느리지만 좋은 성능 기대\n",
    "  - 경사 하강법을 사용하여 트리를 앙상블에 추가\n",
    "  - GradientBoostingClassifier()\n",
    "    * 그레이디언트 부스팅 분류 클래스 생성\n",
    "    * sklearn.ensemble 내에 존재\n",
    "    * 매개변수\n",
    "      - loss : 손실 함수 지정\n",
    "        * 기본값은 로지스틱 손실함수를 의미하는 deviance\n",
    "      - learning_rate : 트리가 앙상블에 기여하는 정도를 조절 (default 0.1)\n",
    "      - n_estimators : 부스팅 단계를 수행하는 트리의 개수 (default 100)\n",
    "      - subsample : 사용할 훈련 세트의 샘플 비율 지정 (default 1.0)\n",
    "      - max_depth : 개별 회귀 트리의 최대 깊이 (default 3)\n",
    "\n",
    "* 히스토그램 기반 그레이디언트 부스팅\n",
    "  - 그레이디언트 부스팅의 속도를 개선한 것\n",
    "  - 입력 특성을 256개의 구간으로 나누어, 노드를 분할할 때 최적의 분할을 매우 빠르게 찾는다.\n",
    "  - 256개의 구간 중에서 하나를 떼어놓고 누락된 값을 위해서 사용\n",
    "  - HistgradientBoostingClassifier()\n",
    "    * 히스토그램 기반 그레이디언트 부스팅 분류 클래스 생성\n",
    "    * sklearn.ensemble 내에 존재\n",
    "    *  사용하려면 sklearn.experimental 내에 있는 enable_hist_gradient_boosting 모듈을 import\n",
    "    * 매개변수\n",
    "      - learning_rate : 학습률 또는 감쇠율 (default 1.0)\n",
    "      - max_iter : 부스팅 단계를 수행하는 트리의 개수 (default 100)\n",
    "      - max_bins : 입력 데이터를 나눌 구간의 개수 (default 255로, 이보다 작은 값이 가능함, 누락된 값을 위한 구간 1개 존재)\n",
    "  - permultation_importance()\n",
    "    * 특성을 하나씩 랜덤하게 섞어서 모델의 성능이 변하는지 관찰하여 어떤 특성이 중요한지 계산\n",
    "    * 히스토그램 기반 그레이디언트 부스팅의 특성 중요도를 계산하기 위해 사용\n",
    "\n",
    "* 그레이디언트 부스팅 (사이킷런X)\n",
    "  - XGBClassifier()\n",
    "    * xgboost 내에 존재\n",
    "    * tree_method 매개변수를 hist로 지정하면 히스토그램 기반 그레이디언트 부스팅 사용\n",
    "  - LGBMClassifier()\n",
    "    * lightgbm 내에 존재"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
