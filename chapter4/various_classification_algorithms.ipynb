{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 다양한 분류 알고리즘"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-1. 로지스틱 회귀\n",
    "* 확률 구하기 (K-최근접 이웃)\n",
    "  - 데이터 준비하기\n",
    "    * read_csv('파일명.csv') : csv 파일을 데이터프레임으로 읽을 때 사용\n",
    "    * head() 함수 : 데이터프레임의 행을 출력할 때 사용\n",
    "    * unique() 함수 : 선택한 열의 고유한 값을 추출\n",
    "  - 훈련 세트와 테스트 세트로 분류\n",
    "    * train_test_split() 함수\n",
    "      - 데이터를 훈련 세트와 테스트 세트로 분류\n",
    "      - 매개변수 random_state : 랜덤 시드 설정\n",
    "  - 데이터 표준화\n",
    "    * StandardScaler() 함수\n",
    "      - 데이터를 표준화 전처리할 때 사용\n",
    "  - K-최근접 이웃 분류기\n",
    "    * KNeighborsClassifier() \n",
    "      - K-최근접 이웃 분류기 생성\n",
    "      - 속성 classes_ : 타깃 값이 정렬되어 있는 속성\n",
    "  - 학습, 데이터 값, 정확도 예측\n",
    "    * fit() : 학습\n",
    "    * predict() : 타깃 값을 예측할 때 사용\n",
    "    * predict_proba() : 클래스 별 확률 값을 반환\n",
    "    * kneighbors() : 이웃 거리와 이웃 샘플의 인덱스 반환\n",
    "\n",
    "* 분류\n",
    "  - 이진 분류 : 타깃 값이 2개인 분류\n",
    "  - 다중 분류 : 타깃 값이 여러 개(3개 이상)인 분류\n",
    "\n",
    "* 로지스틱 회귀\n",
    "  - 선형 방정식을 사용한 분류 알고리즘\n",
    "  - K개의 설명 변수가 주어졌을 때 어떤 이벤트 A 가 발생할 확률 P(X)\n",
    "  - $$P(X) = \\beta_0 + \\beta_1X_1 + ... + \\beta_kX_k + randomerror(\\epsilon)$$\n",
    "  - $$범위 : -\\infty ~ \\infty$$\n",
    "  - 로지스틱 회귀는 선형 회귀와 달리 시그모이드 함수나 소프트맥스 함수 등의 활성 함수를 사용하여 클래스 확률 출력\n",
    "  - LogisticRegression()\n",
    "    * 선형 분류 알고리즘인 로지스틱 회귀를 위한 클래스\n",
    "    * sklearn.linear_model 내에 존재\n",
    "    * 속성\n",
    "      - classes_ : 타깃 값이 배열로 저장\n",
    "      - coef_ : 특성에 대한 계수를 포함한 배열\n",
    "      - intercept_ : 절편 값이 저장됨\n",
    "    * 매개변수\n",
    "      - solver : 사용할 알고리즘 선택\n",
    "        * default : lbfgs\n",
    "        * sag : 확률적 평균 경사 하강법으로, 특성과 샘플 수가 많을 때 성능이 좋다.\n",
    "      - penalty : 규제 선택\n",
    "        * l1 : 라쏘 방식\n",
    "        * l2 : 릿지 방식 (default)\n",
    "      - c : 규제 강도 제어 (default : 1.0)\n",
    "        * 작을수록 규제가 강해진다.\n",
    "    * decision_function() 함수 : z 값 계산 가능\n",
    "\n",
    "* 시그모이드 함수 (Sigmoid)\n",
    "  - 선형 방정식의 출력을 0과 1 사이의 값으로 압축하여 이진 분류를 위해 사용\n",
    "  - (P(X)가 -무한대일때 0, +무한대일 때 1)\n",
    "  - 로짓 변환\n",
    "    * 오즈에 자연 로그를 취해 logit 함수를 생성\n",
    "    * 오즈 : 성공과 실패의 비율 (P/(1-P))\n",
    "    * $$ logit(P) = ln\\frac{P}{1-P} = f(x)$$\n",
    "    * $$ P = \\frac{\\exp^{f(x)}}{1+\\exp^{f(x)}} = \\frac{1}{1+\\exp^{-f(x)}}$$\n",
    "  - expit()\n",
    "    * 사이파이에서 제공하는 시그모이드 함수\n",
    "    * scipy.special 내에 존재\n",
    "\n",
    "* 소프트맥스 함수 (Softmax)\n",
    "  - 다중 분류에서 여러 선형 방정식의 출력 결과를 정규화하여 합이 1이 되도록 만든다.\n",
    "  - $$y_k = \\frac{\\exp{a_k}}{\\sum_{i=1}^{n}\\exp{a_i}}$$\n",
    "  - softmax()\n",
    "    * 사이파이에서 제공하는 소프트맥스 함수\n",
    "    * scipy.special 내에 존재"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-2. 확률적 경사 하강법\n",
    "* 점진적인 학습\n",
    "  - 앞서 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 더 훈련\n",
    "  - 대표적인 점진적 학습 알고리즘은 확률적 경사 하강법\n",
    "\n",
    "* 확률적 경사 하강법\n",
    "  - 훈련 세트에서 샘플 하나씩 꺼내 손실 함수의 경사를 따라 최적의 모델을 찾는 알고리즘\n",
    "  - 에포크\n",
    "    * 확률적 경사 하강법에서 전체 샘플을 모두 사용하는 한 번 반복을 의미\n",
    "    * 일반적으로 확률적 경사 하강법 알고리즘은 수십에서 수백 번의 에포크 반복\n",
    "  - 미니 배치 경사 하강법\n",
    "    * 샘플을 하나씩 사용하지 않고, 여러 개를 사용해 경사 하강법 수행\n",
    "  - 배치 경사 하강법\n",
    "    * 한번에 전체 샘플을 사용한 경사 하강법 수행\n",
    "    * 극단적으로 한 번 경사로를 따라 이동하기 위해 전체 샘플을 사용\n",
    "\n",
    "* 손실 함수\n",
    "  - 어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지 측정하는 기준\n",
    "  - 비용함수와 손실함수의 차이 \n",
    "    * 비용함수 : 훈련 세트에 있는 모든 샘플에 대한 손실함수의 합\n",
    "    * 손실함수 : 샘플 하나에 대한 손실을 정의\n",
    "  - 확률적 경사 하강법이 최적화할 대상\n",
    "  - 각 문제에 따른 손실함수\n",
    "    * 이진 분류 : 로지스틱 회귀(or 이진 크로스엔트로피) 손실함수\n",
    "    * 다중 분류 : 크로스엔트로피 손실함수\n",
    "    * 회귀 문제 : 평균 제곱 오차 손실함수\n",
    "\n",
    "* 로지스틱 손실 함수\n",
    "  - 이진 분류에서 쓰이므로 이진 크로스엔트로피 손실함수라 함\n",
    "  - 각 타깃에 따른 손실 함수\n",
    "    * 타깃 1일 때 손실함수 : -log(예측확률)\n",
    "      - 확률이 1에서 멀어질수록 손실은 아주 큰 양수가 된다.\n",
    "    * 타깃 0일 떄 손실함수 : -log(1-예측확률)\n",
    "      - 확률이 0에서 멀어질수록 손실은 아주 큰 양수가 된다.\n",
    "\n",
    "* SGDClassifier()\n",
    "  - 사이킷런에서 확률적 경사하강법을 사용한 분류 모델을 만든다.\n",
    "  - 매개변수\n",
    "    * loss : 확률적 경사 하강법으로 최적화할 손실함수 지정\n",
    "      - hinge : 서포트 벡터 머신을 위한 손실함수 (default)\n",
    "      - log : 로지스틱 회귀를 위한 손실함수\n",
    "    * penalty : 규제의 종류 지정\n",
    "      - l1 : 라쏘 규제\n",
    "      - l2 : 릿지 규제 (default)\n",
    "    * alpha : 규제 강도 지정 (default : 0.0001)\n",
    "    * max_iter : 에포크 횟수 지정 (default : 1000)\n",
    "    * tol : 반복을 멈출 조건 (default : 0.001)\n",
    "      - n_iter_no_change 에서 지정한 에포크 동안 손실이 tol 만큼 줄어들지 않으면 알고리즘 중단 (default : 5)\n",
    "  - partial_fit() : 1 에포크씩 이어서 훈련 가능\n",
    "\n",
    "* SGDRegressor()\n",
    "  - 사이킷런에서 확률적 경사하강법을 사용한 회귀 모델을 만든다.\n",
    "  - 매개변수는 SGDClassifier과 동일하나, loss의 default : squared_loss이다.\n",
    "\n",
    "* 에포크의 과대/과소 적합\n",
    "  - 에포크 횟수에 따라 과대/과소 적합 가능성이 있다.\n",
    "    * 횟수가 적을 때 -> 과소적합\n",
    "    * 횟수가 많을 때 -> 과대 적합\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
