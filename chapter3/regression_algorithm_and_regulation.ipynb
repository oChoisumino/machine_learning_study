{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 회귀 알고리즘과 모델 규제"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. k-최근접 이웃 회귀\n",
    "* 지도 학습\n",
    "  - 입력과 타깃을 전달하여 모델을 훈련한 후, 새로운 데이터를 예측하는 데 활용\n",
    "  - 크게 분류와 회귀로 나뉜다.\n",
    "    * 분류 : 샘플을 몇 개의 클래스 중 하나로 분류\n",
    "    * 회귀 : 임의의 수치를 예측\n",
    "\n",
    "* K-최근접 이웃 회귀\n",
    "  - 1. 분류\n",
    "    * 예측하려는 샘플에 가장 가까운 샘플 K개를 선택하여, 이중 다수의 클래스를 새로운 샘플의 클래스로 예측\n",
    "    * KNeighborsClassifier()\n",
    "  - 2. 회귀\n",
    "    * 예측하려는 샘플에 가장 가까운 샘플 K개를 선택하여, 이들의 평균값으로 새로운 샘플의 값을 예측\n",
    "    * KNeighborsRegressor()\n",
    "\n",
    "* 데이터 준비\n",
    "  - scatter() 함수\n",
    "    * matplotlib.pyplot 내에 존재\n",
    "    * 산점도를 그리는 함수\n",
    "  - train_test_split() 함수\n",
    "    * sklearn.model_selection 내에 존재\n",
    "    * 데이터를 훈련 세트와 테스트 세트로 나눈다.\n",
    "  - reshape() 함수\n",
    "    * 배열의 크기와 차원 변경 가능\n",
    "    * -1을 지정할 경우 크기가 자동으로 지정되어, 나머지 원소 개수로 채운다.\n",
    "\n",
    "* 결정 계수 (R^2)\n",
    "  - 회귀 문제의 대표적인 성능 측정 도구\n",
    "  - $$ R^2 = 1 - \\frac{(타깃-예측)^2 의 합}{(타깃-평균)^2 의 합}$$\n",
    "  - 1에 가까울수록 성능이 좋고, 0에 가까울수록 성능이 나쁜 모델\n",
    "\n",
    "* 평균 절대 오차 (MAE)\n",
    "  - 모델에서 예측하는 함수의 값과 실제 값의 편차에 절댓값을 취해 모두 더한 다음 평균을 낸 값\n",
    "  - $$MAE = \\frac{1}{N}\\sum_{i=1}^{n}|\\hat{y_i} - y_i|$$\n",
    "  - mean_absolute_error() 함수 사용\n",
    "\n",
    "* 평균 제곱 오차\n",
    "  - 편차의 제곱을 모두 더한 값의 평균을 의미\n",
    "  - $$ MSE = \\frac{1}{N}\\sum_{i=1}^{n}(\\hat{y_i} - y_i)^2$$\n",
    "  - mean_squared_error() 함수 사용\n",
    "\n",
    "* 과대 적합 vs 과소 적합\n",
    "  - 과대 적합\n",
    "    * 모델의 훈련 세트 성능이 테스트 세트 성능보다 훨씬 높은 경우\n",
    "    * 모델이 훈련 세트에 너무 집착해서 데이터에 내제된 거시적인 패턴을 감지하지 못함\n",
    "    * 훈련 세트에만 잘 맞는 모델이라 새로운 샘플에 대한 예측이 올바르지 않음\n",
    "  - 과소 적합\n",
    "    * 훈련 세트와 테스트 성능이 모두 낮거나, 테스트 세트 성능이 훈련 세트 성능보다 높을 때의 경우\n",
    "    * 모델이 너무 단순하여 훈련 세트에 적절히 훈련되지 않은 경우"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. 선형 회귀\n",
    "* K-최근접 이웃의 한계\n",
    "  - K-최근접 이웃 회귀는 가장 가까운 샘플을 찾아 타깃을 평균하므로, 훈련 세트의 범위를 벗어나면 엉뚱한 값 예측할 수 있다.\n",
    "\n",
    "* 선형 회귀\n",
    "  - 특성과 타깃의 관계를 가장 잘 나타내는 선형 방적식을 찾는다.\n",
    "  - 선형 회귀가 찾은 특성과 타깃 사이의 관계는 선형 방정식의 계수 또는 가중치에 저장\n",
    "  - 특성이 하나인 경우 어떤 직선을 학습하는 알고리즘\n",
    "  - 모델 파라미터\n",
    "    * 머신러닝 알고리즘이 찾은 값\n",
    "    * 머신러닝 알고리즘이 특성에서 학습한 파라미터\n",
    "  - LinearRegression() 함수\n",
    "    * sklearn.linear_model 내에 존재\n",
    "    * 선형 회귀 모델\n",
    "    * 매개변수\n",
    "      - fit_intercept : 절편 학습 여부를 결정하는 매개변수(default : True)\n",
    "    * 모델 파라미터\n",
    "      - coef_ :(coefficient) : 계수 또는 가중치\n",
    "      - intercept_ (intercept) : 절편\n",
    "\n",
    "* 다항 회귀\n",
    "  - 다항식을 사용하여 특성과 타깃 사이의 관계를 나타낸다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-3. 특성 공학과 규제\n",
    "* 다중 회귀\n",
    "  - 여러 개의 특성을 사용한 선형 회귀 모델\n",
    "  - 1차원 : 선, 2차원 : 면, 3차원 : 공간\n",
    "\n",
    "* 특성 공학\n",
    "  - 기존의 특성을 사용해 새로운 특성을 뽑아내는 작업\n",
    "  - 주어진 특성을 조합하여 새로운 특성을 만드는 일련의 작업 과정\n",
    "\n",
    "* 판다스(Pandas)\n",
    "  - 유명한 데이터 분석 라이브러리\n",
    "  - 데이터프레임 : 판다스의 핵심 데이터 구조\n",
    "  - read_csv()\n",
    "    * csv 파일을 읽어 판다스 데이터프레임으로 변환하는 함수\n",
    "    * 매개변수\n",
    "      - sep : csv 파일의 구분자를 지정 (default : ,)\n",
    "      - header : 데이터프레임에서 열 이름으로 사용할 csv 파일의 행 번호 지정 (default : 첫번째 행)\n",
    "      - skiprows : 파일에서 읽기 전에 건너뛸 행 개수 지정\n",
    "      - nrows : 파일에서 읽을 행의 개수 지정\n",
    "  - to_numpy()\n",
    "    * 데이터프레임을 넘파이 배열로 변환\n",
    "\n",
    "* 사이킷런의 변환기\n",
    "  - PolynomialFeatures()\n",
    "    * PolynomialFeatures 클래스를 만드는 함수\n",
    "    * 주어진 특성을 조합해서 새로운 특성을 만든다.\n",
    "    * 기본적으로 각 특성을 제곱한 항과 서로 곱한 항을 추가한다. (절편인 1도 추가됨)\n",
    "    * 매개변수\n",
    "      - include_bias : 절편을 위한 특성을 추가할 유무를 의미 (default : True)\n",
    "      - interaction_only : 거듭제곱항은 제외되고 특성 간의 곱셈 항만 추가할 지 여부 (default : False)\n",
    "      - degree : 최고 차수를 지정 (default : 2)\n",
    "    * get_feature_names_out()\n",
    "      - 각각의 특성이 어떤 조합으로 만들어졌는가 출력\n",
    "\n",
    "* 규제\n",
    "  - 머신러닝 모델이 훈련 세트를 과도하게 학습하지 못하도록 훼방하는 것\n",
    "  - 모델이 과대적합 되지 않게 만드는 것\n",
    "  - 특성의 스케일이 정규화되지 않으면 곱해지는 계수가 차이남 -> 크기가 달라 공정하게 제어되지 않아 정규화 필요\n",
    "  - StandardScaler()\n",
    "    * StandardScaler 클래스 객체를 생성하는 함수\n",
    "    * 특성값을 표준 점수로 변환하는 함수\n",
    "  - 규제 방법 구분\n",
    "    * 릿지 : 계수를 제곱한 값을 기준으로 규제 적용\n",
    "    * 라쏘 : 계수의 절댓값을 기준으로 규제 적용\n",
    "\n",
    "* 릿지 회귀 (Ridge)\n",
    "  - 규제가 있는 선형 회귀 모델 중 하나\n",
    "  - 선형 모델의 계수를 작게 만들어 과대 적합을 완화시킴\n",
    "  - Ridge()\n",
    "    * 규제가 있는 회귀 알고리즘인 릿지 회귀 모델을 훈련\n",
    "    * 매개변수\n",
    "      - alpha : 규제의 강도를 조절 (default : 1)\n",
    "      - solver : 최적의 모델을 찾기 위한 방법 지정 (default : auto)\n",
    "        * auto : 데이터에 따라 자동으로 선택\n",
    "        * seg : 확률적 평균 경사 하강법으로, 특성과 샘플수가 많을 때 성능이 빠름\n",
    "      - random_state : solver가 seg, sega일 때 넘파이 난수 시드값 지정\n",
    "  - 하이퍼파라미터\n",
    "    * 머신러닝 모델이 학습할 수 없고, 직접 전달해야 하는 파라미터\n",
    "\n",
    "* 라쏘 회귀 (Lasso)\n",
    "  - 규제가 있느 선형 회귀 모델\n",
    "  - 릿지와 다르게 계수를 아예 0으로 만들 수 있다.\n",
    "  - Lasso()\n",
    "    * 매개변수\n",
    "      - alpha : 규제의 강도를 조절\n",
    "      - random_state : 난수 시드값 지정\n",
    "      - max_iter : 알고리즘의 수행 반복 횟수 지정(default : 1000)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
